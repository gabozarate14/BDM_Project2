import configparser
import pyspark
from pyspark import SparkConf
from pyspark.sql import SparkSession
import os
import sys
from datetime import datetime
import json
import happybase

CONFIG_ROUTE = 'utils/config.cfg'
MODEL_ROUTE = '../model/'


def readFormatted(table):
    hdfs_path = config.get('hdfs_formatted', table)
    directory = f"hdfs://{host}:27000/{hdfs_path}"
    df = spark.read.parquet(directory)
    return df.rdd.map(lambda r: r.asDict()).cache()


def extract_tuples(tuple_with_tuples):
    extracted_tuple = tuple()
    for element in tuple_with_tuples:
        if isinstance(element, tuple):
            extracted_tuple += extract_tuples(element)
        else:
            extracted_tuple += (element,)
    return extracted_tuple


if __name__ == '__main__':
    # Get the parameters
    config = configparser.ConfigParser()
    config.read(CONFIG_ROUTE)
    # Server info
    host = config.get('data_server', 'host')
    # Spark
    pyspark_python = config.get('pyspark', 'python')
    pyspark_driver_python = config.get('pyspark', 'driver_python')
    # Hadoop
    hadoop_home = config.get('hadoop', 'home')

    # HDFS
    host = config.get('data_server', 'host')
    user = config.get('data_server', 'user')
    hdfs_path = config.get('routes', 'hdfs')

    # Set Spark
    os.environ["HADOOP_HOME"] = hadoop_home
    sys.path.append(hadoop_home + "\\bin")
    os.environ["PYSPARK_PYTHON"] = pyspark_python
    os.environ["PYSPARK_DRIVER_PYTHON"] = pyspark_driver_python

    # Set Spark
    conf = SparkConf() \
        .set("spark.master", "local") \
        .set("spark.app.name", "Exploitation Zone - Model") \
        .set("spark.executor.memory", "8g") \
        .set("spark.driver.memory", "4g") \
        .set("spark.sql.shuffle.partitions", "4") \
        .set("spark.default.parallelism", "8") \
 \
    # Create the session
    spark = SparkSession.builder \
        .config(conf=conf) \
        .getOrCreate()

    income_rdd = readFormatted('income')
    idealista_rdd = readFormatted('idealista')
    neighborhood_rdd = readFormatted('neighborhood')

    # Collect the RDD to accelerate the calculations
    income_rdd.collect()
    idealista_rdd.collect()
    neighborhood_rdd.collect()

    neigh_join = neighborhood_rdd.map(lambda r: (r['_id'], r['name'])).cache()

    # KPI 4: Average sales price x neighborhood

    avg_sale_price_x_neighborhood = idealista_rdd.filter(lambda r: r['operation'] == 'sale').map(
        lambda r: (r['idNeighborhood'], (r['price'], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).map(
        lambda r: (r[0], r[1][0] / r[1][1]))

    # KPI 5: Average RFD x neighborhood

    avg_rfd_x_neighborhood = income_rdd.map(lambda r: (r['idNeighborhood'], (r['RFD'], 1))).reduceByKey(
        lambda a, b: (a[0] + b[0], a[1] + b[1])).map(
        lambda r: (r[0], r[1][0] / r[1][1]))

    # predictive_rdd will contain KPI 4 and 5 joined to predict RFD based on sale

    predictive_rdd = avg_sale_price_x_neighborhood.join(avg_rfd_x_neighborhood).join(neigh_join).map(
        lambda r: (r[1][1], r[1][0][0], r[1][0][1]))

    df_predictive = spark.createDataFrame(predictive_rdd,
                                          ["Neighborhood", "Price", "RFD"])

    folder_path = config.get('model_folder', 'path')
    output_directory = f'{folder_path}/avg_rfd_x_neighborhood'

    df_predictive.write.mode("overwrite").csv(output_directory, header=True)

    spark.stop()
