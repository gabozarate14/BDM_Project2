import configparser
import pyspark
from pyspark import SparkConf
from pyspark.sql import SparkSession
import os
import sys
from datetime import datetime
import json
import happybase

CONFIG_ROUTE = 'utils/config.cfg'
MODEL_ROUTE = '../model/'


def readFormatted(connection, table_name):
    table = connection.table(table_name)
    rdd = spark.sparkContext.parallelize(list(table.scan())).map(
        lambda r: json.loads(r[1][b'cf:value'].decode())).cache()
    return rdd


def formatPredKPI1(listDict):
    dict1 = listDict[0]
    dict2 = listDict[1]
    # KPI = RFD / Total. Euros/m2 construït
    try:
        kpi = round(dict1['RFD'] / float(dict2['Total. Euros/m2 construït']), 4)
    except:
        kpi = 0

    ans = (
        dict1['year'],
        dict1['idDistrict'],
        dict1['idNeighborhood'],
        dict1['RFD'],
        dict1['pop'],
        dict2['Nou. Euros/m2 construït'],
        dict2["Nou. Milers d'euros"],
        dict2['Total. Euros/m2 construït'],
        dict2["Total. Milers d'euros"],
        dict2['Usat. Euros/m2 construït'],
        dict2["Usat. Milers d'euros"],
        kpi
    )

    return ans


def extract_tuples(tuple_with_tuples):
    extracted_tuple = tuple()
    for element in tuple_with_tuples:
        if isinstance(element, tuple):
            extracted_tuple += extract_tuples(element)
        else:
            extracted_tuple += (element,)
    return extracted_tuple


if __name__ == '__main__':
    # Get the parameters
    config = configparser.ConfigParser()
    config.read(CONFIG_ROUTE)
    # Server info
    host = config.get('data_server', 'host')
    # Spark
    pyspark_python = config.get('pyspark', 'python')
    pyspark_driver_python = config.get('pyspark', 'driver_python')
    # Hadoop
    hadoop_home = config.get('hadoop', 'home')
    # Hbase tables
    idealista_table_name = config.get('hbase', 'idealista_table')
    income_table_name = config.get('hbase', 'income_table')
    price_table_name = config.get('hbase', 'price_table')
    district_table_name = config.get('hbase', 'district_table')
    neighborhood_table_name = config.get('hbase', 'neighborhood_table')

    # HDFS
    host = config.get('data_server', 'host')
    user = config.get('data_server', 'user')
    hdfs_path = config.get('routes', 'hdfs')

    # Connect to HBase
    connection = happybase.Connection(host=host, port=9090)
    connection.open()

    # Set Spark
    os.environ["HADOOP_HOME"] = hadoop_home
    sys.path.append(hadoop_home + "\\bin")
    os.environ["PYSPARK_PYTHON"] = pyspark_python
    os.environ["PYSPARK_DRIVER_PYTHON"] = pyspark_driver_python

    # Set Spark
    conf = SparkConf() \
        .set("spark.master", "local") \
        .set("spark.app.name", "Exploitation Zone") \
        .set("dfs.client.max.block.acquire.failures", "100")

    # Create the session
    spark = SparkSession.builder \
        .config(conf=conf) \
        .getOrCreate()

    income_rdd = readFormatted(connection, income_table_name)
    price_rdd = readFormatted(connection, price_table_name)
    idealista_rdd = readFormatted(connection, idealista_table_name)
    neighborhood_rdd = readFormatted(connection, neighborhood_table_name)
    district_rdd = readFormatted(connection, district_table_name)

    dist_join = district_rdd.map(lambda r: (r['_id'], r['name'])).cache()
    neigh_join = neighborhood_rdd.map(lambda r: (r['_id'], r['name'])).cache()

    # KPI 1: RFD / Total. Euros/m2 construït
    kpi1_rdd = income_rdd.filter(lambda r: r['idNeighborhood'] and r['idNeighborhood']).map(lambda r: (
        (str(r['year']) if r['year'] else '') + (r['idNeighborhood'] if r['idNeighborhood'] else ''), r)).join(
        price_rdd.map(
            lambda r: (
                (str(r['Any']) if r['Any'] else '') + (r['idNeighborhood'] if r['idNeighborhood'] else ''), r))).map(
        lambda r: formatPredKPI1(r[1])).map(lambda r: (r[1], r)).join(dist_join).map(
        lambda r: (r[1][0][2], r)).join(neigh_join).map(lambda r: extract_tuples(r)).map(
        lambda r:tuple(r[i] for i in (2, 14, 15, 5, 6, 7, 8, 9, 10, 11, 12, 13)))
        # lambda r: tuple(str(v) if v != 'NA' else None for v in r))

    df_kpi1 = spark.createDataFrame(kpi1_rdd,
       ['Year',
        'District',
        'Neighborhood',
        'RFD',
        'Population',
        'New Euros/m2 built',
        "New Thousand of Euros",
        'Total Euros/m2 built',
        "Total Thousand of Euros",
        'Used Euros/m2 built',
        "Used Thousand of Euros",
        'Kpi'])

    output_directory = f"hdfs://{host}:27000/{hdfs_path}/model/kpi1"
    # output_directory = '../output/kpi1'
    df_kpi1.write.csv(output_directory, header=True)

    # KPI 2: Average income x year
    avg_income_x_year = income_rdd.filter(lambda x: x["idDistrict"] and x["idNeighborhood"]).map(
        lambda x: ((x["year"], x["idDistrict"], x["idNeighborhood"]), (x["RFD"], 1))).reduceByKey(
        lambda a, b: (a[0] + b[0], a[1] + b[1])).map(
        lambda x: extract_tuples((x[0], x[1][0] / x[1][1]))).map(lambda r: (r[1], r)).join(dist_join).map(
        lambda r: (r[1][0][2], r)).join(neigh_join).map(lambda r: extract_tuples(r)).map(
        lambda r: (str(r[2]), r[6], r[7], str(r[5])))

    output_directory = f"hdfs://{host}:27000/{hdfs_path}/dashboard/avg_income_x_year"
    # output_directory = '../output/avg_income_x_year'
    df_avg_income_x_year = spark.createDataFrame(avg_income_x_year,
                                                 ["Year", "District", "Neighborhood", "RFD"])
    df_avg_income_x_year.write.csv(output_directory, header=True)


    # KPI 3. Average flat sale price x month
    avg_flat_sale_price_x_month = idealista_rdd.filter(
        lambda r: r['propertyType'] == 'flat' and r['operation'] == 'sale' and r["idDistrict"] and
                  r["idNeighborhood"]).map(
        lambda r: (
            (r['extractDate'][0:4], r['extractDate'][5:7], r['idDistrict'], r['idNeighborhood']),
            (r['price'], 1))).reduceByKey(
        lambda a, b: (a[0] + b[0], a[1] + b[1])).map(
        lambda x: extract_tuples((x[0], x[1][0] / x[1][1]))).map(lambda r: (r[2], r)).join(dist_join).map(
        lambda r: (r[1][0][3], r)).join(neigh_join).map(lambda r: extract_tuples(r)).map(
        lambda r: (str(r[2]), str(r[3]), r[7], r[8], str(r[6])))

    output_directory = f"hdfs://{host}:27000/{hdfs_path}/dashboard/df_avg_flat_sale_price_x_month"
    # output_directory = '../output/df_avg_flat_sale_price_x_month'
    df_avg_flat_sale_price_x_month = spark.createDataFrame(avg_flat_sale_price_x_month,
                                                           ["Year", "Month", "District", "Neighborhood", "Price"])
    df_avg_flat_sale_price_x_month.write.csv(output_directory, header=True)

    # KPI 4: Average sales price x neighborhood

    avg_sale_price_x_neighborhood = idealista_rdd.filter(lambda r: r['operation'] == 'sale').map(
        lambda r: (r['idNeighborhood'], (r['price'], 1))).reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])).map(
        lambda r: (r[0], r[1][0] / r[1][1]))

    # KPI 5: Average RFD x neighborhood

    avg_rfd_x_neighborhood = income_rdd.map(lambda r: (r['idNeighborhood'], (r['RFD'], 1))).reduceByKey(
        lambda a, b: (a[0] + b[0], a[1] + b[1])).map(
        lambda r: (r[0], r[1][0] / r[1][1]))

    # predictive_rdd will contain KPI 4 and 5 joined to predict RFD based on sale

    predictive_rdd = avg_sale_price_x_neighborhood.join(avg_rfd_x_neighborhood).join(neigh_join).map(
        lambda r: (r[1][1], r[1][0][0], r[1][0][1]))

    df_predictive = spark.createDataFrame(predictive_rdd,
                            ["Neighborhood", "Price", "RFD"])

    output_directory = f"hdfs://{host}:27000/{hdfs_path}/model/avg_rfd_x_neighborhood"
    # output_directory = '../output/avg_rfd_x_neighborhood'

    df_predictive.write.csv(output_directory, header=True)

    connection.close()
    spark.stop()
