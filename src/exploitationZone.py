import configparser
import pyspark
from pyspark import SparkConf
from pyspark.sql import SparkSession
import os
import sys
from datetime import datetime
import json
import happybase

CONFIG_ROUTE = 'utils/config.cfg'


def readFormatted(connection, table_name):
    table = connection.table(table_name)
    rdd = spark.sparkContext.parallelize(list(table.scan())).map(
        lambda r: json.loads(r[1][b'cf:value'].decode())).cache()
    return rdd


if __name__ == '__main__':
    # Get the parameters
    config = configparser.ConfigParser()
    config.read(CONFIG_ROUTE)
    # Server info
    host = config.get('data_server', 'host')
    # Spark
    pyspark_python = config.get('pyspark', 'python')
    pyspark_driver_python = config.get('pyspark', 'driver_python')
    # Hadoop
    hadoop_home = config.get('hadoop', 'home')
    # Hbase tables
    idealista_table_name = config.get('hbase', 'idealista_table')
    income_table_name = config.get('hbase', 'income_table')
    price_table_name = config.get('hbase', 'price_table')
    district_table_name = config.get('hbase', 'district_table')
    neighborhood_table_name = config.get('hbase', 'neighborhood_table')

    # Connect to HBase
    connection = happybase.Connection(host=host, port=9090)
    connection.open()

    # Set Spark
    os.environ["HADOOP_HOME"] = hadoop_home
    sys.path.append(hadoop_home + "\\bin")
    os.environ["PYSPARK_PYTHON"] = pyspark_python
    os.environ["PYSPARK_DRIVER_PYTHON"] = pyspark_driver_python

    # Set Spark
    conf = SparkConf() \
        .set("spark.master", "local") \
        .set("spark.app.name", "Exploitation Zone")

    # Create the session
    spark = SparkSession.builder \
        .config(conf=conf) \
        .getOrCreate()

    income_rdd = readFormatted(connection, income_table_name)
    idealista_rdd = readFormatted(connection, idealista_table_name)

    print(idealista_rdd.count())
    for f in idealista_rdd.collect():
        print(f)

    connection.close()
